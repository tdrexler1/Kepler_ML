---
title: "DS 740 - Final Project"
author: "Timothy Drexler"
date: "8/8/2019"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# used to load large csv file
require(readr)
# used to manipulate data frame
require(dplyr)
# used for plots
require(ggplot2)
# used for plot colors
require(RColorBrewer)
# used to arrange plots
require(grid); require(gridExtra)
# used to melt data for plotting
require(data.table)
# used to plot correlation matrix
require(corrplot)
# used for PCA biplot
require(ggfortify)

# prevent masking of 'dplyr' select by other packages
select <- dplyr::select
```

```{r message=FALSE}
# create data set 
koi_df <- 
  # load data set
  data.frame(read_csv("cumulative.csv", n_max = 9564) ) %>% 
  # select response + useful predictors (exclude error columns, flags, ids, etc.)
  select(koi_pdisposition, koi_period, koi_impact, koi_duration, koi_depth, koi_prad, koi_teq, koi_insol, koi_model_snr, koi_steff, koi_slogg, koi_srad, koi_kepmag ) %>%
  # include only complete cases
  filter(complete.cases(.)) %>%
  # response as factor
  mutate( koi_pdisposition = as.factor(koi_pdisposition) )
```

## EXPLORATION OF PREDICTOR VARIABLES


## VARIABLE TRANSFORMATION
```{r}
# no variables have negative values, but there are some zeroes, so use log(x+1) transformation on all variables other than 'koi_kepmag'
koi_transform <- 
  koi_df %>% mutate_at(vars(-c(koi_pdisposition, koi_kepmag) ), list(~log1p(.) ) )
```

```{r fig.height=10, fig.width=10}
# principal components analysis

# principal components of data set with transformed variables
koi_pca <- prcomp(koi_transform[,-1], center=T, scale=T)

# proportion of variance explained for each component
summary(koi_pca)
koi_pca$rotation
# PCA biplot with ggplot 
g <- autoplot(koi_pca, loadings = T, label = F, scale=0, loadings.label = T, loadings.colour = "orange2", loadings.label.colour = "orange2") +
  scale_y_continuous(
    sec.axis = sec_axis(~ . * 0.01 ) ) +
  scale_x_continuous(
    sec.axis = sec_axis(~ . * 0.01 ) ) +
  theme(axis.ticks.y.right = element_line(color = "orange2"),
        axis.ticks.x.top = element_line(color="orange2"),
        axis.text.y.right = element_text(color="orange2"),
        axis.text.x.top = element_text(color="orange2"),
        plot.title = element_text(hjust = 0.5)) +
  labs(title = "Principle Component Biplot of Kepler Data")

### THIS NEEDS WORK
loading_labels <- layer_data(g, 3)
g + geom_label(data = loading_labels, aes(fill = "black", colour = "white") )


# 'koi_kepmag' and 'koi_steff' appear to have lowest overall loadings in first two principal components
```

## MODEL SELECTION & SELECTION PROCESS ASSESSMENT

```{r}
# define 'constants'
CVFOLDS_SELECT <- 10 # number of folds used for selection process cross-validation 

CVFOLDS_ASSESS <- 10 # number of folds used for assessment process cross-validation

cv_group_select <- function(n, m=10, seed=NULL){
  # Creates randomly-sampled groups for cross-validation folds.
  # Args: n : # of observations in data frame, m : # of CV folds, seed : initialization value for random-number generator
  # Returns: vector of length n of numbers 1 to m in random order

  # vector (size n) of group labels
  groups_select <- c( rep(1:m, n%/%m) )
  if(n%%m!=0){groups_select <- c(groups_select, 1:(n%%m) )}
  
  # initialize random number generator
  set.seed(seed)
  
  # return randomly sampled groups
  return( sample( groups_select, size = n ) )
}
```

```{r}
### TESTING DATA SET
select_df <- koi_transform[1:500,]

model_select_cvgroups <- cv_group_select(n = dim(select_df)[1], m = CVFOLDS_SELECT, seed = 8)

obs_response <- select_df$koi_pdisposition
```


```{r}
# support vector machine
library(e1071)
  
  gamma_values <- c(0.5, 1, 2, 3, 4)
  cost_values <- c(0.01, 0.1, 1, 10, 100, 1000)
  
  svm_predictions <- rep(NA, dim(select_df)[1])
  svm_er_matrix <- matrix(NA, nr=length(gamma_values), nc=length(cost_values) )
  
  for (g in 1:length(gamma_values) ){
    for (c in 1:length(cost_values) ){
      for (i in 1:CVFOLDS_SELECT){
        
        groupi <- model_select_cvgroups == i
        
        svm_fit <- svm(koi_pdisposition ~ ., data = select_df[!groupi, ], kernel = "radial", cost = cost_values[c], gamma = gamma_values[g])
        
        svm_predictions[groupi] <- predict(svm_fit, select_df[groupi, ])
      }
      
      # confusion matrix of predicted responses vs. observed responses
    svm_conf_mtrx <- table(svm_predictions, obs_response)
  
    svm_er_matrix[g,c] <- sum(svm_conf_mtrx[row(svm_conf_mtrx) != col(svm_conf_mtrx)]) / sum(svm_conf_mtrx)
    
    
    }
  }
  
  svm_min_er_indices = which(svm_er_matrix==min(svm_er_matrix, na.rm = T), arr.ind = T)
  svm_gamma_best <- gamma_values[svm_min_er_indices[1, 1] ]
  svm_cost_best <- cost_values[svm_min_er_indices[1, 2] ]
  svm_min_er <- svm_er_matrix[svm_min_er_indices][1]
svm_min_er
```


```{r}

# 'full' model (all potential predictors)
koi_model_full <- 
  glm(koi_pdisposition ~ ., family = "binomial", data = koi_transform)

# 'null' model (intercept only)
koi_model_null <- 
  glm(koi_pdisposition ~ 1, family = "binomial", data = koi_transform)

# forward step wise selection
koi_model_step_forward <- 
  step(koi_model_null, 
       scope = list(lower=koi_model_null, upper=koi_model_full), 
       direction = "forward")

# backward step wise selection
koi_model_step_backward <- 
   step(koi_model_full, direction = "backward")

koi_model_step_backward$call
koi_model_step_forward$call

```

```{r}
library(glmulti)

  select_df_boost <- 
    select_df %>% 
    mutate( koi_pdisposition = ifelse(koi_pdisposition=="CANDIDATE", 1, 0) )

glmulti.lm.out <- 
  glmulti(koi_pdisposition ~ ., data = select_df_boost, 
               level=1, 
               method = "h", 
               crit = "aic", 
               confsetsize = 5,
               plotty = F, report = F,
               fitfunction = "glm",
               family = binomial)
glmulti.lm.out@formulas
summary(glmulti.lm.out@objects[[1]])
```
***remove 'koi_impact' ***
```{r}
# logistic regression

threshold_values <- seq(0.01, 1, 0.01)

glm_predictions <- rep(NA, dim(select_df)[1])
glm_er_vector <- rep(NA, length(threshold_values) )

for (t in 1:length(threshold_values) ){
  for (i in 1:CVFOLDS_SELECT){
    
    groupi <- model_select_cvgroups == i
    
    glm_fit <- glm(koi_pdisposition ~ ., data = select_df[!groupi, ], family = "binomial")
    
    glm_predictions[groupi] <- predict(glm_fit, newdata = select_df[groupi, ], type = "response")
  }
  
  glm_conf_mtrx <- table(glm_predictions > threshold_values[t], obs_response)
  
  glm_er_vector[t] <- sum(glm_conf_mtrx[row(glm_conf_mtrx) != col(glm_conf_mtrx)]) / sum(glm_conf_mtrx)
}

glm_t_best = threshold_values[which.min(glm_er_vector)] 
glm_min_er = min(glm_er_vector)

glm_min_er
```

```{r}
library(pROC)
glm_roc <- roc(response = select_df$koi_pdisposition, predictor = glm_predictions)
plot.roc(glm_roc)
glm_roc$auc
  
```



```{r}
# knn
library(class)

  # k-nearest neighbors 'k' values
  k_values <- 1:30
  
  # initialize vectors for storing model results
  knn_predictions <- rep(NA, dim(select_df)[1])
  knn_er_vector <- rep(NA, length(k_values) )
  
  # iterate over all values of k
  for (k in 1:length(k_values) ){
    # iterate over all CV folds
    for (i in 1:CVFOLDS_SELECT){
      
      groupi <- model_select_cvgroups == i
      
      # matrix of training set predictors
      knn_train_obs <- scale(select_df[!groupi, -1])
      
      # vector of training set response
      knn_train_response <- select_df[!groupi, 1]
      
      # matrix of validation set predictors
      knn_valid_obs <- scale(select_df[groupi, -1],
                             center = attr(knn_train_obs, "scaled:center"),
                             scale = attr(knn_train_obs, "scaled:scale")
                             )

      # vector of validation set response 
      knn_valid_response <- select_df[groupi, 1]
      
      # k-nearest neighbors predictions on validation set
      knn_predictions[groupi] <- knn(knn_train_obs, knn_valid_obs, knn_train_response, k = k_values[k])
      
    } # end iteration over CV folds
      
    # confusion matrix of predicted responses vs. observed responses
    knn_conf_mtrx <- table(knn_predictions, obs_response)
  
    # calculate error rate of model predictions for each value of 'k'
    knn_er_vector[k] <- sum(knn_conf_mtrx[row(knn_conf_mtrx) != col(knn_conf_mtrx)]) / sum(knn_conf_mtrx)
      
  } # end iteration over values of k

  knn_k_best = k_values[which.min(knn_er_vector)] # value of k with minimum classification error rate
  knn_min_er = min(knn_er_vector) # minimum classification error rate
  
  knn_min_er
```


```{r}
# pruned decision tree
library(tree)

leaves_values <- 2:20

  # initialize vector for storing AUC measurements for each value of 'best'
  #tree_prune_AUC_vector <- rep(NA, (length(leaves_values)+1) )
  tree_er_vector <- rep(NA, (length(leaves_values)+1) )
  tree_predictions <- rep(NA, dim(select_df)[1])
  
  # iterate over all values of 'best'
  for (l in 1:length(leaves_values) ){
    # iterate over all CV folds
    for (i in 1:CVFOLDS_SELECT){
      
      groupi <- model_select_cvgroups == i
      
      # fit initial decision tree model to training data & store model summary
      tree_00 <- tree(koi_pdisposition ~ ., data = select_df[!groupi, ] )
      tree_sum <- summary(tree_00)
      
      # check to ensure number of terminal nodes in potential pruned tree is less-than or equal to number of terminal nodes in initial tree
      if(leaves_values[l] <= tree_sum$size){
        tree_prune <- prune.misclass(tree_00, best=leaves_values[l])
      } else {
        tree_prune <- prune.misclass(tree_00, best=tree_sum$size)
      } # end ifelse
      
      # predicted classifications of test data
      tree_predictions[groupi] <- predict(tree_prune, newdata = select_df[groupi, ], type="class")
      
    } # end iteration over CV folds
    
    # confusion matrix of predicted responses vs. observed responses
    tree_conf_mtrx <- table(tree_predictions, obs_response)
    
    # calculate error rate of model predictions for each value of 'best'
    tree_er_vector[l+1] <- sum(tree_conf_mtrx[row(tree_conf_mtrx) != col(tree_conf_mtrx)]) / sum(tree_conf_mtrx)
    
  } # end iteration over values of 'best'
  
  tree_l_best = leaves_values[which.min(na.omit(tree_er_vector))] # value of 'best' parameter for model with maximum AUC
  tree_min_er = min(na.omit(tree_er_vector)) # classification error rate of model with maximum AUC
  
  tree_min_er
```



```{r}
# random forest / bagging
library(randomForest)
  
  mtry_values <- 1:12  

  rf_er_vector <- rep(NA, length(mtry_values) )
  rf_predictions <- rep(NA, dim(select_df)[1])
  
  # set random number generator for reproducible results
  set.seed(8)
  
  # iterate over all values of 'mtry'
  for (p in 1:length(mtry_values) ){
    # iterate over all CV folds
    for (i in 1:CVFOLDS_SELECT){
      
      groupi <- model_select_cvgroups == i
      
      # fit random forest model to training data
      rf_fit <- randomForest(koi_pdisposition ~ . , data = select_df[!groupi, ], mtry=mtry_values[p], strata = koi_pdisposition)
      
      # predicted classifications of test data
      rf_predictions[groupi] <- predict(rf_fit, newdata = select_df[groupi, ], type="response")
      
    } # end iteration over CV folds  
    
    # confusion matrix of predicted responses vs. observed responses
    rf_conf_mtrx <- table(rf_predictions, obs_response)
  
    # calculate error rate of model predictions for each value of 'mtry'
    rf_er_vector[p] <- sum(rf_conf_mtrx[row(rf_conf_mtrx) != col(rf_conf_mtrx)]) / sum(rf_conf_mtrx)
  
  } # end iteration over values of 'mtry'
  
  rf_mtry_best = mtry_values[which.min(rf_er_vector)] # value of 'best' parameter for model with maximum AUC
  rf_min_er = min(rf_er_vector) # classification error rate of model with maximum AUC
  
  rf_min_er
```


```{r}
# boosting
library(gbm)
### TODO: select classification threshold parameter value

  # boosting 'shrinkage' parameter
  lambda_values <- seq(0.001, 0.01, by = 0.001)
  # boosting 'interaction.depth' parameter
  depth_values <- seq(1, 3)
  # boosting 'n.trees' parameter
  tree_values <- seq(500, 1000, by=500)
  
  # response variable as binary numeric
  select_df_boost <- 
    select_df %>% 
    mutate( koi_pdisposition = ifelse(koi_pdisposition=="CANDIDATE", 1, 0) )
  
  # matrix for storing predicted classification probabilities
  boost_predictions <- rep(NA, dim(select_df_boost)[1]  )
  
  # array for storing classification error rates for each value set of boosting parameters
  boost_er_array <-
    array(NA, dim = c(length(lambda_values), length(depth_values), length(tree_values) ) ,
          dimnames = list(lambda_values, depth_values, tree_values) )
  
  # iterate over all values of 'n.trees'
  for(t in 1:length(tree_values) ){
    # iterate over all values of 'interaction.depth'
    for(d in 1:length(depth_values) ){
      # iterate over all values of 'shrinkage'
      for(l in 1:length(lambda_values) ){
        # iterate over all CV folds
        for(i in 1:CVFOLDS_SELECT){
          
          groupi <- model_select_cvgroups == i
          
          # fit boost model to training data
          boost_model <- gbm(koi_pdisposition ~ .,
                             data = select_df_boost[!groupi, ],
                             distribution = "bernoulli",
                             n.trees = tree_values[t],
                             shrinkage = lambda_values[l],
                             interaction.depth = depth_values[d])
          
          # matrix of predicted classification probabilities of test data
          boost_predictions[groupi] <- predict(boost_model, newdata = select_df_boost[groupi, ], n.trees = tree_values[t], type = "response") 
          
        } # end iteration over CV folds
        
        # confusion matrix of predicted responses vs. observed responses
        boost_conf_mtrx <- table(boost_predictions>0.5, select_df_boost$koi_pdisposition)
        
        # error rate of model predictions for each set of boosting parameters
        boost_er_array[l, d, t] <- sum(boost_conf_mtrx[row(boost_conf_mtrx) != col(boost_conf_mtrx)]) / sum(boost_conf_mtrx)
        
      } # end iteration over lambda values
    } # end iteration over interaction depth
  } # end iteration over number of trees
  
  # array indices of model with minimum classification error rate
  boost_min_er_indices = which(boost_er_array==min( boost_er_array, na.rm = T), arr.ind = T)
  
  # minimum classification error rate
  boost_min_er = boost_er_array[boost_min_er_indices][1]
  
  # 'shrinkage' parameter for model with minimum classification error rate
  boost_lambda_best = lambda_values[boost_min_er_indices[1, 1] ]
  
  # 'interaction.depth' parameter for model with minimum classification error rate
  boost_depth_best = depth_values[boost_min_er_indices[1, 2] ]
  
  # 'n.trees' parameter for model with minimum classification error rate
  boost_trees_best = tree_values[boost_min_er_indices[1, 3] ]

# reporting
boost_min_er
```

```{r}
# ann
library(nnet)

  # ann 'size' parameter (number of hidden nodes)
  node_values <- 1:20
  # store classification error rates for each 'size'
  ann_er_vector <- rep(NA, length(node_values) )
  # store classification predictions
  ann_predictions <- rep(NA, dim(select_df)[1])
  # random number initialization for reproducible results
  set.seed(42)

  # iterate over all values of 'size'
  for (n in 1:length(node_values) ){
    # iterate over all CV folds
    for (i in 1:CVFOLDS_SELECT){
      
      groupi <- model_select_cvgroups == i
      
      # fit ann to training data
      ann_fit <- nnet(koi_pdisposition ~ ., data = select_df[!groupi, ], size = node_values[n], maxit = 200, trace = F)
      # predicted classifications of test data
      ann_predictions[groupi] <- predict(ann_fit, select_df[groupi, ], type = "class")
    
    } # end iteration over CV folds
    
    # confusion matrix of predicted responses vs. observed responses
    ann_conf_mtrx <- table(ann_predictions, obs_response)
  
    # error rate of model predictions for each value of 'size'
    ann_er_vector[n] <- sum(ann_conf_mtrx[row(ann_conf_mtrx) != col(ann_conf_mtrx)]) / sum(ann_conf_mtrx)
    
  } # end iteration over 'size' values

  ann_nodes_best = node_values[which.min(ann_er_vector)] # 'size' with minimum classifcation error rate
  ann_min_er = min(ann_er_vector) # minimum classification error rate

# reporting
ann_min_er
```

```{r}
# penalized regression (Ridge regression / LASSO / elastic net) with discrete family

library(glmnet)
s_values = c(0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5) 
alpha_values = c(0, 0.25, 0.5, 0.75, 1)
cv_values = 1:CVFOLDS_SELECT

glmnet_er_matrix <- matrix(NA, nr = length(alpha_values), nc = length(s_values) )
glmnet_best_lambdas_vector <- rep(NA, length(alpha_values) )
glmnet_predictions <- matrix(NA, nr = dim(select_df)[1], nc = length(s_values) )

  # initialize array for storing classification error rates for each value set of boosting parameters
  glmnet_er_array <-
    array(NA, dim = c(length(alpha_values), length(s_values), length(cv_values) ) ,
          dimnames = list(alpha_values, s_values, cvs = cv_values ) )


for (a in 1:length(alpha_values) ){
    for(i in 1:CVFOLDS_SELECT){
      
      groupi <- model_select_cvgroups == i
      
    x_vars <- as.matrix(select_df[!groupi, -1])
    y_var <- as.matrix(select_df[!groupi, 1])
    x_new <- as.matrix(select_df[groupi, -1])

    glmnet_fit <- glmnet(x_vars, y_var, family = "binomial", lambda=s_values, alpha = alpha_values[a])
    
    glmnet_predictions <- predict(glmnet_fit, newx = x_new, type = "class")
    
    for(s in 1:length(s_values) ){
        glmnet_conf_mtrx <- table(glmnet_predictions[ ,s], select_df$koi_pdisposition[groupi])
        glmnet_er_array[a,s,i] <- sum(glmnet_conf_mtrx[row(glmnet_conf_mtrx) != col(glmnet_conf_mtrx)]) / sum(glmnet_conf_mtrx)
        glmnet_er_matrix[a,s] <- mean(glmnet_er_array[a,s,])
        }
    }
  
}
  # find array indices of model that produced maximum AUC value
  glmnet_min_er_indices = which(glmnet_er_matrix==min(glmnet_er_matrix), arr.ind = T)
  
  # maximum AUC value
  glmnet_min_er = glmnet_er_matrix[glmnet_min_er_indices][1]
  
  # value of 'shrinkage' parameter for model with maximum AUC
  glmnet_s_best = s_values[glmnet_min_er_indices[1, 2] ]
  
  # value of 'interaction.depth' parameter for model with maximum AUC
  glmnet_alpha_best = alpha_values[glmnet_min_er_indices[1, 1] ]

```














































###################### NOTES ############################

- binary response, 17 predictors, all continuous

- possible methods:

-- svm - seems to work, 'tune' for cv model selection

-- lda/qda - probably won't work: predictors are not multivariate normal and unequal covariance matrices; same results even after log+1 transform

-- knn - also decent performance

-- trees/random forest/boosting - random forest works best

-- glm/logistic regression - seems reasonable, less accurate than svm

-- ann - decent

-- penalized regression with discrete response - seems to be working, doesn't perform extremely well

- also: 

-- pca - dimension reduction / variable reduction

#########################################################














###################### OLD CODE ############################
```{r}
# lda/qda
library(MASS)

lda_predictions <- rep(NA, dim(select_df)[1])
qda_predictions <- rep(NA, dim(select_df)[1])

for (i in 1:CVFOLDS_SELECT){
  groupi <- model_select_cvgroups == i
  
  lda_fit <- lda(koi_pdisposition ~ ., data = select_df[!groupi, ])
  qda_fit <- qda(koi_pdisposition ~ ., data = select_df[!groupi, ])
  
  lda_predictions[groupi] <- as.character(predict(lda_fit, newdata = select_df[groupi, ])$class)
  qda_predictions[groupi] <- as.character(predict(qda_fit, newdata = select_df[groupi, ])$class)
  
}

lda_conf_mtrx <- table(lda_predictions, obs_response)
qda_conf_mtrx <- table(qda_predictions, obs_response)

lda_er <- sum(lda_conf_mtrx[row(lda_conf_mtrx) != col(lda_conf_mtrx)]) / sum(lda_conf_mtrx)
qda_er <- sum(qda_conf_mtrx[row(qda_conf_mtrx) != col(qda_conf_mtrx)]) / sum(qda_conf_mtrx)
lda_er; qda_er

```

```{r}
# LDA/QDA assumptions

# Bartlett test of homogeneity of variances using significance level alpha = 0.05:
# H-null of equal variances
for(i in 2:13){
  print(bartlett.test(select_df[ , i], select_df[ , 1]) )
}

library(MVN)

mvn(select_df[select_df$koi_pdisposition == "CANDIDATE" , 2:13], mvnTest = "hz")$multivariateNormality
mvn(select_df[select_df$koi_pdisposition == "FALSE POSITIVE" , 2:13], mvnTest = "hz")$multivariateNormality

        
library(biotools)
boxM(select_df[ , 2:13], select_df[ ,1])
```

```{r}
library(bestglm)
data(SAheart)
bestglm(SAheart, IC="CV", CVArgs=list(Method="HTF", K=10, REP=1), family=binomial)
library(gdata)
lbw <- read.xls("http://www.umass.edu/statdata/statdata/data/lowbwt.xls")
?bestglm
glm_test <- glm(koi_pdisposition ~ ., data = select_df, family = "binomial")
summary(glm_test)

select_bestglm <- 
  select_df %>% 
  rename(y = koi_pdisposition) %>% 
  select(koi_period:koi_kepmag, y)
  #mutate(y = ifelse(koi_pdisposition=="CANDIDATE", 1, 0) ) %>% 
  #select(-koi_pdisposition, koi_period:koi_kepmag, y)

koi.bestglm <- bestglm(select_bestglm, family = binomial, IC = "CV", CVArgs=list(Method="HTF", K=10, REP=1), method="exhaustive")

koi.bestglm <- bestglm(select_bestglm, family = binomial, IC = "BIC", method="exhaustive")

koi.bestglm$BestModels

summary(koi.bestglm$BestModel)

koi.bestglm$Subsets


# 'full' model (all potential predictors)
koi_model_full <- 
  glm(y ~ ., family = "binomial", data = select_bestglm)

# 'null' model (intercept only)
koi_model_null <- 
  glm(y ~ 1, family = "binomial", data = select_bestglm)

# forward step wise selection
koi_model_step_forward <- 
  step(koi_model_null, 
       scope = list(lower=koi_model_null, upper=koi_model_full), 
       direction = "forward")

# backward step wise selection
koi_model_step_backward <- 
   step(koi_model_full, direction = "backward")
```

```{r}
# Define a predict() function for regsubsets objects
predict.regsubsets <- function(object, newdata, id, ...){
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newdata)
  coefi = coef(object, id=id)
  xvars = names(coefi)
  mat[ , xvars] %*% coefi
} # end function predict.regsubsets
npv <-  12

bestglm_er_matrix <- matrix(NA , nr=npv, nc=CVFOLDS_SELECT) # row=number of variables, column = which fold

for (i in 1:CVFOLDS_SELECT){
        
        groupi <- model_select_cvgroups == i
  
        bestglm_models <- bestglm(select_bestglm[!groupi,], family = binomial, IC = "AIC", method = "exhaustive")
  
  # #cv.fit = regsubsets(Sales ~ t + Day.of.Week + Bread.Sand.Sold + Wraps.Sold + 
  #                       Muffins.Sold + Cookies.Sold + Fruit.Cup.Sold + Chips + 
  #                       Juices + Sodas + Coffees + Max.Temp + Total.Items.Wasted, 
  #                     data = cafe[!groupi,], nvmax=12)
  
  for(j in 1:npv){
    bestglm_predictions <- predict(bestglm_models$Subsets, newdata = select_bestglm[groupi, ], id=j)
    
    bestglm_conf_mtrx <- table(bestglm_predictions, obs_response)
  
    bestglm_er_matrix[j, i] <- sum(bestglm_conf_mtrx[row(bestglm_conf_mtrx) != col(bestglm_conf_mtrx)]) / sum(bestglm_conf_mtrx)
    #group.error[j, i] = mean( (cafe$Sales[groupi]-y.pred)^2 )
  } # end iter over model size
  
} # end iter over folds

MSE = apply(group.error, 1, mean)
plot(MSE)
which.min(MSE)

se = apply(group.error, 1, sd)/sqrt(k)
se[14]
which(MSE <= MSE[14]+se[14]) # 'near-optimal' models, within 1 standard error of Model 14
```

```{r}
library(glmulti)

glmulti.lm.out <- 
  glmulti(koi_pdisposition ~ ., data = koi_transform, 
               level=1, 
               method = "h", 
               crit = "aic", 
               confsetsize = 5,
               plotty = F, report = F,
               fitfunction = "glm",
               family = binomial)
glmulti.lm.out@formulas
summary(glmulti.lm.out@objects[[1]])
```
